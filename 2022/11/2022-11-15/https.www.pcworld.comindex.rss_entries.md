## Qualcommâ€™s Snapdragon 8 Gen 2 chip leans on AI to supercharge smartphones
 - [https://www.pcworld.com/article/1379806/qualcomm-prioritizes-ai-in-snapdragon-8-gen-2-smartphone-chip.html](https://www.pcworld.com/article/1379806/qualcomm-prioritizes-ai-in-snapdragon-8-gen-2-smartphone-chip.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 23:00:35+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>With AI capabilities &mdash; and not just compute power &mdash; becoming key features of smartphones and even PCs, Qualcomm&rsquo;s new Snapdragon 8 Gen 2 smartphone chip leans heavily on how AI can improve photos and your other mobile experiences, executives said. Key additions include real-time hardware ray tracing, the ability to sense and optimize different &ldquo;layers&rdquo; in your photos, and massive connectivity upgrades whose throughput will likely outpace your home Internet connection.</p>



<p>Specifically, AI will be used to power a number of new experiences in Snapdragon-powered phones, which will debut this fall. Smarter cameras will try to interpret what you&rsquo;re shooting and enhance it before you even take the picture, not afterwards, executives said. What Qualcomm now calls an &ldquo;always sensing&rdquo; camera will also remain in low-power mode, scanning the world around it; you&rsquo;ll be able to hold up the phone to scan an QR code even if the phone is in standby mode, they said. The phone will also use its AI capabilities to improve cellular connections, as previous phones have done.</p>



<p>So far, Qualcomm hasn&rsquo;t said much about the Snapdragon 8 Gen 2 Arm chip, though the company promises more details will be added during its Snapdragon Technology Summit, going on now in Maui, Hawaii. <a href="https://www.pcworld.com/article/556345/qualcomms-next-snapdragon-promises-always-on-smartphone-cameras.html">Qualcomm launched the Snapdragon 8 Gen 1 last year</a>, and the capabilities of the chip, as well as its name, imply an upgrade rather than a generational refresh. </p>



<p>From an overall performance perspective, Qualcomm claims that the Hexagon DSP, which powers its &ldquo;Snapdragon Smart&rdquo; features, will be 4.35 times faster than the Gen 1 chip. Its integrated Kryo CPU cores will be 35 percent faster (and offer 40 percent more power efficiency) while its Adreno GPU will offer up to 25 percent faster performance.</p>



<p>But it&rsquo;s fair to say that the Snapdragon 8 Gen 2 will likely power your next Android smartphone. Qualcomm has relationships with most smartphone suppliers, including Samsung, though the latter was not in a list of names of companies expected to use the new platform. Qualcomm&rsquo;s Snapdragon 8 Gen 2 partners include Asus Republic of Gamers (ROG), Honor, iQoo, Motorola, Nubia, OnePlus, Oppo, Redmagic, Redmi, SHARP, Sony, vivo, Xiaomi, Xingji/Meizu, and ZTE, the company said.</p>



<h2 id="qualcomms-snapdragon-8-gen-2-whats-in-it">Qualcomm&rsquo;s Snapdragon 8 Gen 2: what&rsquo;s in it?</h2>



<p>The design of the Snapdragon 8 Gen 2 slightly differs from Gen 1. Qualcomm didn&rsquo;t break down the details of the individual cores, but executives told reporters that there will be a single 3.19GHz &ldquo;prime&rdquo; core, the <a href="https://www.pcworld.com/article/796784/arm-promises-powerful-new-pcs-with-cortex-x3-immortalis-gpus.html">Arm Cortex-X3</a>, and four performance cores, running at 2.8GHz. Cisco Cheng, the senior director of product marketing for Qualcomm, said that the four performance cores &mdash; which have been rumored to be the Cortex-A715 &mdash; are made up of &ldquo;two distinct architectures&rdquo; that can support both 32-bit and 64-bit applications. Another key difference is that there are only three &ldquo;efficiency&rdquo; cores, rather than four, which are designed for low-power, background tasks.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="Qualcomm Snapdragon 8 Gen 2 blocks" class="wp-image-1380878" height="865" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Snapdragon-8-Gen-2-block-shot.png?w=1200" width="1200" /><figcaption>An overview of the Qualcomm Snapdragon Gen 2 chip.</figcaption></figure><p class="imageCredit">Qualcomm</p></div>



<p>Rival MediaTek, which is vying to push Qualcomm out of premium smartphones, <a href="https://www.pcworld.com/article/919862/mediateks-dimensity-9200-wants-a-seat-at-the-cool-kids-table.html">disclosed its own Dimensity 9200 chip</a> earlier this month. It too, is based on Arm, and its performance core runs slightly slower at 3.05GHz. </p>



<h2 id="ai-powers-photos-and-more">AI powers photos, and more</h2>



<p>Most smartphone chips share some commonalities between PC processors, namely the inclusion of a CPU or compute core, graphics, and wireless I/O. To underscore the point that AI is a key driver of the chip, however, Qualcomm disclosed that it has included what it calls a &ldquo;Hexagon direct link,&rdquo; which basically can use the Hexagon core to inject AI into the chip&rsquo;s other capabilities.</p>



<p>Snapdragon chips already include a Sensing Hub, which controls everything from sensors that detect your presence and gaze, gestures, ultrasound and more. Cheng said that the Sensing Hub&rsquo;s AI capabilities have been doubled again, with a second AI core, also providing 50 percent more memory for processing more information.</p>



<p>It&rsquo;s the camera, though, where AI is typically applied, and where you&rsquo;ll see the most benefit from AI. With the Snapdragon 8 Gen 2, the camera&rsquo;s &ldquo;cognitive&rdquo; image signal processor (or ISP) will run what Qualcomm calls a &ldquo;realtime segmentation filter,&rdquo; where it will try and interpret the various &ldquo;segments&rdquo; of what the camera is seeing. What this will mean, executives said, is that the camera will try to put a photo of a nature scene in its best light as you&rsquo;re previewing it with the camera, understanding the part of the photo that&rsquo;s the &ldquo;sky,&rdquo; the part that might be &ldquo;grass&rdquo; or a &ldquo;face,&rdquo; and so on.</p>



<p>&ldquo;It&rsquo;s trained to understand facial landmarks like face, eyes, and glasses,&rdquo; Cheng said. &ldquo;It can understand the background and then take these layers and adjust it in real time so that you get these compelling images. This is essentially precise Photoshop edits, in real time.&rdquo;</p>



<p>In the Snapdragon 8 Gen 1, the company referred to the camera connection as an &ldquo;always on&rdquo; connection, which had creepy connotations. It doesn&rsquo;t appear that the Snapdragon 8 Gen 2&rsquo;s camera has changed that much &mdash; just the terminology. Still, being able to scan a barcode without logging into the camera, loading the camera app, then scanning the code is a plus.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="Qualcomm Snapdragon 8 Gen 2 camera segmentation" class="wp-image-1380879" height="653" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Snapdragon-8-Gen-2camera-segmentation.png?w=1200" width="1200" /><figcaption>Qualcomm has worked with ArcSoft, as it has in previous chips, to &ldquo;segment&rdquo; the image into various components, which it then recognizes and optimizes.</figcaption></figure><p class="imageCredit">Qualcomm</p></div>



<p>Otherwise, smartphone cameras powered by the Snapdragon 8 Gen 2 will be capable of capturing up to 108 megapixels from a single camera at 30fps, or up to 36Mpixels from all three cameras. Video capture capabilities will include 8K HDR at 30 fps, and slow-mo 720p video capture at 120 fps.</p>



<p>The Snapdragon 8 Gen 2 also applies AI to the wireless connection, including to extend the wireless range and signal strength. Inside the Snapdragon 8 Gen 2 will be the new X70 5G modem. This is the first Snapdragon phone platform that can support 4X carrier aggregation or even use a pair of 5G SIM cards simultaneously &mdash; useful if you&rsquo;re traveling in Taiwan with a dual-SIM phone. In that case, you&rsquo;d be able to receive a 5G call from a relative while using a second, local SIM to surf the Web using that data plan. </p>



<p>Qualcomm&rsquo;s FastConnect Wi-Fi block will support Wi-Fi 7, including what it calls &ldquo;simultaneous high band multi-link&rdquo; &mdash; a smart connectivity solution where your phone will link multiple high-bandwidth channels together, offering up to a whopping 5.8Gpbs (if a nearby Wi-Fi router supports it, of course). Cheng said that using the multiple lanes will also reduce latency to as little as 2ms, ideal for cloud gaming.</p>



<h2 id="just-like-a-pc-snapdragon-ray-tracing">Just like a PC? Snapdragon ray tracing</h2>



<p>While the number of people playing games on mobile phones reaches into the multiple billions, the number of users playing mobile 3D games, or first-person shooters, is somewhat smaller but still sizeable: the mobile version of Player Unknown: Battlegrounds (PUBG) <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://twitter.com/PUBGMOBILE/status/1373786682357657605&amp;xcust=2-1-1379806-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">tops 1 billion downloads</a>. (Analyst firm Newzoo <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.protocol.com/bulletins/mobile-gaming-decline-newzoo-2022&amp;xcust=2-1-1379806-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">says</a> that mobile game revenue will drop by 6 percent this year, however.) That&rsquo;s enough to justify the latest addition to the Snapdragon arsenal: real-time hardware ray tracing. Qualcomm will announce support from several game publishers to build in support for its features, which will include ambient occlusion, reflections, and soft shadows.</p>



<p>The integrated Adreno GPU features both increased performance and power savings of 25 percent, Cheng said. Games that use the Vulcan API (specifically version 1.3), should see a 30 percent performance boost, he said. Cheng said that the graphics upgrades would be paired with dynamic spatial audio, also a first for the Snapdragon platform.</p>



<p>The Adreno chip will also add support for HDR Vivid, a Chinese HDR standard, as well as a technique called OLED aging compensation to prevent burn-in. Snapdragon phones using the new chip will be able to display 4K at 60Hz, or 1440p at 144Hz, the company said.</p>



<p>Qualcomm isn&rsquo;t expected to announce a Snapdragon 7 chip for cheaper smartphones at its Technology Summit, however. Qualcomm typically releases a &ldquo;Plus&rdquo; variant for gaming phones in a few months time.</p>
CPUs and Processors, Smartphones</div>

## Razer software update transforms Blade 14 ports to USB4, quadrupling speeds
 - [https://www.pcworld.com/article/1380916/razer-software-update-transforms-blade-14-ports-to-usb4.html](https://www.pcworld.com/article/1380916/razer-software-update-transforms-blade-14-ports-to-usb4.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 20:10:10+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>If you&rsquo;ve bought a 2022 edition of the Razer Blade 14, you&rsquo;re in for an unexpected bonus: a firmware upgrade that will transform the laptop&rsquo;s USB-C ports into full-fledged USB4 ports, upping their throughput significantly.</p>



<p>Razer said that the unexpected upgrade was already supported by the laptop&rsquo;s existing hardware, but that the upgrade was being made available via AMD&rsquo;s new support for the USB4 standard.</p>



<p>If you visit (and buy) <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.razer.com/gaming-laptops/Razer-Blade-14-/RZ09-0370CEA3-R3U1&amp;xcust=2-1-1380916-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">a Razer Blade 14 on Razer&rsquo;s site</a>, the Razer Blade 14, the laptop is currently advertised as having a &ldquo;2 x USB 3.2 Gen 2 Type-C with Power Delivery and Display Port 1.4&rdquo; ports, which would normally deliver 10Gbps on each port. Now, thanks to the upgrade, each port will be capable of 40Gbps, via USB4. That&rsquo;s enough to connect <a href="https://www.pcworld.com/article/393714/best-thunderbolt-docks-for-a-laptop-pc.html#:~:text=The%20best%20Thunderbolt%20docks%201%20IOGear%20Quantum%20Dual,Best%20Thunderbolt%20dock%20for%20business%20...%20More%20items">a Thunderbolt dock</a> (that supports USB4) or a <a href="https://www.pcworld.com/article/394176/the-first-usb4-products-are-here-what-to-know-what-to-buy.html">dedicated USB4 peripheral</a> like a fast SSD or multiple 4K displays. Before the upgrade, connecting those external devices wasn&rsquo;t possible.</p>



<p>Improving a laptop port&rsquo;s capabilities via a software upgrade should be impossible. But a spokesman for Razer said that, &ldquo;the existing hardware already resident in the Blade 14 is capable of supporting USB4, but the feature was not initially present due to software limitations. Those limitations have been resolved, hence a firmware update is all that is required to leverage the full USB4 standard.&rdquo;</p>



<p>In addition, the firmware update will turn on Microsoft&rsquo;s Pluton technology, a security core that is available in AMD&rsquo;s Ryzen processors (which power the Blade 14) but not Intel&rsquo;s Core chips. <a href="https://www.pcworld.com/article/393727/microsoft-pluton-will-build-xbox-security-into-amd-intel-qualcomm-cpus.html">Pluton has been used in the Xbox game console to prevent hacking</a>, and was <a href="https://www.pcworld.com/article/621767/why-the-biggest-laptop-vendors-are-ignoring-microsofts-pluton-security-tech.html">adopted by AMD (but not Intel)</a> to protect PCs.</p>



<p>The only wrinkle appears to be that Razer is recommending users <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://mysupport.razer.com/app/warranty-support&amp;xcust=2-1-1380916-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">directly contact Razer&rsquo;s support</a> for the upgrade, rather than use the built-in upgrade tools within the laptop. Nevertheless, upgrading laptop hardware via software isn&rsquo;t something you see every day.</p>
Laptops</div>

## Is this Dell laptop hiding the future of PC memory? Meet CAMM
 - [https://www.pcworld.com/article/1380608/is-this-dell-laptop-hiding-the-future-of-pc-memory.html](https://www.pcworld.com/article/1380608/is-this-dell-laptop-hiding-the-future-of-pc-memory.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 15:44:24+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>If you look at the exterior of the <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.dell.com/en-us/shop/dell-laptops/precision-7670-workstation/spd/precision-16-7670-laptop/xctop7670usvp&amp;xcust=2-1-1380608-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">Dell Precision 7670</a>, you might mistake it for a chunky corporate laptop from 15 years ago. There&rsquo;s nothing in the dull grey exterior that indicates it&rsquo;s hiding technology that might just revolutionize the PC market. But crack that sucker open with Gordon in the latest PCWorld video, and you might just be shocked at what&rsquo;s hiding inside. </p>



<p>This laptop is one of the first models on the market to use a brand new standard for memory, CAMM, <a href="https://www.pcworld.com/article/693366/dell-defends-its-controversial-new-laptop-memory.html">which we&rsquo;ve covered in depth</a> when the concept was introduced. That stands for Compression Attached Memory Module, which replaces the DIMM and SO-DIMM standard that&rsquo;s been used in laptops and desktops for decades. The new design has a few advantages. While it&rsquo;s physically larger than a piece of SO-DIMM memory, it&rsquo;s more spread out, allowing for up to 128GB of memory on a single circuit board. </p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure><p>The memory and connection plate are also far, far thinner, with electrical contacts that look more like a processor socket than a traditional memory slot. That&rsquo;s a huge advantage for laptop builders, which have been leaning into soldered RAM instead of user-accessible SO-DIMM slots for years just to save a few millimeters of Z-height. Dell&rsquo;s design also minimizes the physical distance between the memory and the CPU. </p>



<p>The CAMM design is a little less user-friendly than conventional RAM modules, as it requires you to remove six screws and a stiffening plate (at least on this version) instead of simply popping off a couple of retention clips. But for anyone who feels comfortable popping off the back of their laptop, it&rsquo;s still fairly straightforward. </p>



<p>While the CAMM system is a Dell patent and technically a proprietary format, the company is trying to work with Joint Electron Device Engineering Council (JEDEC) to license it as a memory standard. Whether any of Dell&rsquo;s competitors will be willing to hop on the bus (har har) remains to be seen. For more explainers on the latest in PC technology, be sure to <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.youtube.com/@pcworld&amp;xcust=2-1-1380608-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow" target="_blank">subscribe to PCWorld on YouTube!</a></p>
Laptops</div>

## This RTX 3070-powered Gigabyte laptop with a 240Hz screen is just $1,100
 - [https://www.pcworld.com/article/1380523/this-gigabyte-laptop-is-powerful-fast-and-551-off.html](https://www.pcworld.com/article/1380523/this-gigabyte-laptop-is-powerful-fast-and-551-off.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 15:27:22+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>Calling all gamers! We&rsquo;ve got a great deal for you today. eBay&rsquo;s selling the <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.ebay.com/itm/125534677240?mkevt=1&amp;mkcid=1&amp;mkrid=711-53200-19255-0&amp;campid=5338189085&amp;toolid=10001&amp;customid=111346X1569483X3ef818195ba8f573254f6bc79f3324ed&amp;xcust=2-1-1380523-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">Gigabyte Aorus for $1,099.99</a>, which is a savings of $551. Not only does this machine have powerful components, but the display&rsquo;s bezels are so skinny they&rsquo;re practically non-existent. Let&rsquo;s crack open the hood and take a look then, shall we?</p>



<p>The Aorus is rocking an Intel Core i7-11800H CPU, an Nvidia GeForce RTX 3070 CPU, 16GB of RAM, and 1TB of SSD storage.The 15.6-inch display has a resolution of 1920&times;1080 and a refresh rate of 240Hz. That means you can expect smooth visuals as well as a decent picture. </p>



<p>We&rsquo;d recommend jumping on this deal sooner rather than later, as six units have sold in the last 24 hours. We don&rsquo;t expect this one to last forever.</p>


<p class="cta wp-block wp-block-button"><a class="cta__btn" href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.ebay.com/itm/125534677240?mkevt=1&amp;mkcid=1&amp;mkrid=711-53200-19255-0&amp;campid=5338189085&amp;toolid=10001&amp;customid=111346X1569483X3ef818195ba8f573254f6bc79f3324ed&amp;xcust=2-1-1380523-7-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow" target="_blank">Get the Gigabyte Aorus for $1,099.99 at eBay</a></p>
Laptops</div>

## 5 must-know GeForce RTX 4080 facts: Overpriced by $500
 - [https://www.pcworld.com/article/1379747/tested-nvidia-geforce-rtx-4080-facts.html](https://www.pcworld.com/article/1379747/tested-nvidia-geforce-rtx-4080-facts.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 14:00:00+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>Nvidia&rsquo;s new GeForce RTX 4080 Founders Edition hits stores tomorrow, lightning-fast performance and a massive new price tag in tow. While last generation&rsquo;s <a href="https://www.pcworld.com/article/393472/nvidia-geforce-rtx-3080-founders-edition-review.html">awesome RTX 3080</a> cost $700 at launch, delivering a monument 60 to 80 percent performance uplift over its predecessor, this new model is releasing with a staggering $1,200 price tag. Yes, that&rsquo;s right: a $500 (or <em>71 percent</em>) increase over the cost of its predecessor. For roughly 30 percent more performance. </p>



<p>Sheesh. We&rsquo;ve been putting the GeForce RTX 4080 through its paces and while Nvidia&rsquo;s new GPU is plenty fast, blowing past last gen&rsquo;s RTX 3090, that new price tag casts a long shadow over its release, to such an extent that we can&rsquo;t even come <em>close </em>to recommending this card at this aggressively insulting price. Look for our comprehensive RTX 4080 gaming performance review to land in the coming days. Today, here&rsquo;s a high-level look at what you need to know before dropping $1,200 (or, ideally, <em>not</em>) on one of these beasts.</p>



<h2 id="1-the-geforce-rtx-4080-chews-through-games">1: The GeForce RTX 4080 chews through games</h2>



<p>There&rsquo;s no denying that the GeForce RTX 4080 is fast. Averaged across our 11-game testing suite&mdash;running a variety of graphics APIs, game engines, and game types&mdash;the GeForce RTX 4080 winds up about 7.5 percent faster than the vaunted RTX 3090, and 30.3 percent faster than the RTX 3080 (though exact performance uplift can vary wildly depending on the game). Unlike last generation&rsquo;s cards, it has no problem hitting the hallowed 60 frames per second mark at 4K resolution with all the eye candy cranked, even in tough titles like <em>Cyberpunk 2077</em>.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><a href="https://www.pcworld.com/article/1348123/nvidia-geforce-rtx-4090-review-ada-lovelace.html"><img alt="Combined total FPS across 11 games" class="wp-image-1379750" height="1200" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/11-game-combined-fps.jpg?quality=50&amp;strip=all&amp;w=1184" width="1184" /></a><figcaption>We used the same game suite for the RTX 4080 as we did in our 4090 review, but with <em>Call of Duty: Modern Warfare 2 </em>added.</figcaption></figure><p class="imageCredit">Brad Chacos/IDG</p></div>



<p>The same holds true for ray-traced and DLSS-enhanced games. As we saw with the <a href="https://www.pcworld.com/article/1348123/nvidia-geforce-rtx-4090-review-ada-lovelace.html">$1,600 GeForce RTX 4090</a>, Nvidia&rsquo;s new &ldquo;Ada Lovelace&rdquo; architecture seriously stepped up the quality of its dedicated ray tracing and tensor cores to deliver unparalleled performance in these cutting-edge games&mdash;especially in the handful of titles that support DLSS 3&rsquo;s Frame Generation feature, which adds AI-generated visuals every other frame to supercharge visual speeds (but <em>not</em> necessarily responsiveness, because those AI frames don&rsquo;t respond to your inputs like a normal frame would). At 1440p in <em>F1 22</em>, the RTX 4080 hits 118fps with standard DLSS 2 on and ray tracing active, and a whopping 234fps with DLSS 3 FG enabled!</p>



<h2 id="2-the-geforce-rtx-4080-chews-through-content-creation">2: The GeForce RTX 4080 chews through content creation</h2>



<p>If you use your graphics card for work as well as play, you&rsquo;ll like what&rsquo;s on offer here&mdash;maybe. Adam &ldquo;EposVox&rdquo; Taylor, the internet&rsquo;s stream professor, already tested the RTX 4080&rsquo;s workstation chops in a gauntlet of content creation benchmarks. Bottom line? The huge performance benefits he <a href="https://www.pcworld.com/article/1375901/tested-nvidias-geforce-rtx-4090-is-a-content-creation-monster.html">first witnessed in the RTX 4090</a> still hold true here, with the RTX 4080 (usually) <em>smoking</em> its last-gen predecessors in streaming, editing, AI art generation, and more.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure><p>But there&rsquo;s a catch: The RTX 4080 only comes with 16GB of onboard VRAM. That&rsquo;s plenty for gaming, of course, but EposVox found it wanting for serious video editors. Check out <a href="https://www.pcworld.com/article/1378383/nvidia-geforce-rtx-4080-content-creation-review.html">his full eval</a> for dozens upon dozens of benchmarks, but here&rsquo;s the key excerpt on why you may want to spend up for a 4090&rsquo;s 24GB capacity or wait for a cheaper RTX 40-series option instead:</p>



<blockquote class="wp-block-quote"><p>&ldquo;Throwing the card in my Threadripper Pro build to text my intense 8K project full of 8K RAW footage, Resolve SuperScaled 4K footage, plugins and effects&hellip; unfortunately the 4080 couldn&rsquo;t export this, just like the previous 80-tier cards I tested. 16GB of VRAM still isn&rsquo;t enough, I suppose&mdash;plugins error out, color graded footage takes longer than the 4090, and so on. 8K is tough, but for $1,200 I would hope at this point to start to see higher VRAM capacities when <a href="https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-xe-hpg-review.html">Intel is handing out $350 Arc graphics cards with 16GB</a>.&rdquo;</p></blockquote>



<h2 id="3-its-huge">3: It&rsquo;s huge</h2>



<p>One of the perks of Nvidia&rsquo;s RTX 3080 Founders Edition was its size. Virtually all custom 3080 boards by the likes of EVGA, Asus, MSI, <em>et cetera</em> were massive three- or four-slot cards that required lots of room in your PC case. The RTX 3080 Founders Edition stood out by being a standard two-slot card that fit into virtually any system, and it looked damned good doing it thanks to Nvidia&rsquo;s stunning industrial design. Yay!</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="Nvidia GeForce RTX 4080 Founders Edition" class="wp-image-1379444" height="800" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/sadfgasdgf.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Brad Chacos/IDG</p></div>



<p>Not so with the RTX 4080 Founders Edition. Nvidia decided to give the RTX 4080 FE the same humongous triple-slot cooler as the RTX 4090 this time around, which means it&rsquo;ll slip into far fewer systems. It&rsquo;s a bit of a perplexing design choice, since the more efficient Lovelace-based RTX 4080 actually draws <em>less</em> power than the RTX 3080 FE under full load in our tests. Nvidia could&rsquo;ve chosen a much cheaper, more manageable two-slot design yet again, I suspect&mdash;though having a huge, silent, and frigid three-slot cooler might help make you feel better about dropping $1,200 on this card I guess?</p>



<h2 id="4-it-uses-a-controversial-and-ugly-power-adapter">4: It uses a controversial (and ugly) power adapter</h2>



<p>Similarly, the GeForce RTX 4080 also joins the RTX 4090 in using a new power adapter designed to connect its new-breed 12VHPWR power port to three standard 8-pin power connectors. I think these short-cabled adapters are fugly in your case, but they&rsquo;re nothing new&mdash;last-gen RTX 30-series Founders Edition cards also shipped with slightly different 12-pin adapters. What <em>is </em>new are dozens of reports of <a href="https://www.pcworld.com/article/1372988/heres-what-we-know-about-the-geforce-rtx-4090-12vhpwr-cables-melting.html">melting 12VHPWR adapters on the RTX 4090</a>. Yikes.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="12VHPWR melted adapter cable reddit" class="wp-image-1361510" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/10/12VHPWR-melted-adapter-cable-reddit.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><a class="imageCredit" href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.reddit.com/r/nvidia/comments/yc6g3u/rtx_4090_adapter_burned/&amp;xcust=2-1-1379747-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow" target="_blank">Reddit user reggie_gakil</a></div>



<p>This isn&rsquo;t anything I&rsquo;d personally worry about. I&rsquo;ve had no problems with my 4080 or 4090 FE, and a few dozen incidents over presumably tens (hundreds?) of thousands of RTX 4090 sales indicates a clear problem, but not a pervasive one. And despite extensive testing,&nbsp;<a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.youtube.com/watch?v=EIKjZ1djp8c&amp;xcust=2-1-1379279-1-0-0&amp;sref=https://www.pcworld.com/article/1379279/nvidia-is-still-working-on-those-melting-rtx-4090-power-cables.html" rel="noreferrer noopener" target="_blank">GamersNexus</a>&nbsp;and&nbsp;<a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.igorslab.de/en/adapter-of-the-gray-analyzed-nvidias-brand-hot-12vhpwr-adapter-with-built-in-breakpoint/&amp;xcust=2-1-1379279-1-0-0&amp;sref=https://www.pcworld.com/article/1379279/nvidia-is-still-working-on-those-melting-rtx-4090-power-cables.html" rel="noreferrer noopener" target="_blank">Igor&rsquo;s Labs</a>&nbsp;have been unable to replicate the melted cables seen by a handful of new RTX 4090 owners.&nbsp;Hopefully Nvidia figures out the root cause, and it&rsquo;s limited to a bad batch of cables or connectors on 4090s alone.</p>



<p>But here, it&rsquo;s another potential point of concern in a product full of tiny concerns. And a $1,200 graphics cards really shouldn&rsquo;t stress you out. Which brings us to the main point.</p>



<h2>5: The GeForce RTX 4080 costs <em>waaaaaaaay</em> too much</h2>



<p>In no world should you pay $1,200 for the RTX 4080. And I suspect Nvidia wants exactly that.</p>



<p>Our full benchmark results won&rsquo;t be posted until our final RTX 4080 review launches in the next day or two&mdash;a burst pipe in the middle of the night foiled my release-day ambitions&mdash;but at a high level, it&rsquo;s about 30 percent faster than the RTX 3080 at 4K resolution with settings maxed out and ray tracing/DLSS off. That&rsquo;s a solid, if unspectacular generational jump (the RTX 4090&rsquo;s 55 to 83 percent leap over the 3090 was <em>much</em> more impressive, as was the 3080 vs. 2080), the sort you&rsquo;d expect to see in the same class moving from one gen to the next. <em>But that&rsquo;s at the same price</em>.</p>



<p>Again, Nvidia is asking for <em>71 percent more cash</em> for that mere 30 percent uplift&mdash;$699 versus $1,199. At the same $1,199 price, the RTX 4080 is only faster than the RTX 3080 Ti by single-digit percentages. That&rsquo;s absolutely insane and aggressively insulting to PC gamers. You should stay far, far away from this graphics card at this price. </p>



<p>There are wider economic reasons why Nvidia probably priced the RTX 4080 this way. The RTX 4090 remains a great option even at $1,600; it can max out a high-refresh rate 4K monitor and chew through content creation tasks without breaking a sweat, something no other GPU can claim, for just $100 more than the RTX 3090&rsquo;s launch MSRP. It&rsquo;s pricey, but good.</p>



		<div class="wp-block-product-widget-block product-widget">
			<div class="product-widget__block-title-wrapper">
				<h4 class="product-widget__block-title" id="the-rtx-4080s-terrible-price-makes-the-rtx-4090-look-even-better">
					The RTX 4080's terrible price makes the rtx 4090 look even better				</h4>
			</div>

			<div class="product-widget__content-wrapper">
									<div class="product-widget__title-wrapper">
						<h3 class="product-widget__title" id="geforce-rtx-4090-founders-edition">GeForce RTX 4090 Founders Edition</h3>
					</div>
				
									<div class="product-widget__image-outer-wrapper">
						<div class="product-widget__image-wrapper">
							<img alt="GeForce RTX 4090 Founders Edition" class="product-widget__image" height="1667" src="https://b2c-contenthub.com/wp-content/uploads/2022/10/Nvidia-GeForce-RTX-4090-1.jpg?quality=50&amp;strip=all" width="2500" /></div>
					</div>
				
									<div class="review product-widget__review-details">
						<img alt="Editors' Choice" class="product-widget__review-details--editors-choice-logo" src="https://www.pcworld.com/wp-content/uploads/2021/09/PC-ED-CHOICE.png" /><div class="product-widget__rating-and-review-link">
								<div class="product-widget__review-details--rating">
											<div class="starRating"></div>
										</div>									<a class="product-widget__review-link" href="https://www.pcworld.com/article/1348123/nvidia-geforce-rtx-4090-review-ada-lovelace.html" target="_blank">Read our review</a>
									
							</div>
											</div>
				
				<div class="product-widget__information">
				
									</div>
			</div>
		</div>

		


<p>The RTX 4080 is pricey but does nothing to justify its downright ludicrous cost. It <em>does</em>, however, manage to make both the RTX 4090 and the piles of older RTX 30-series cards look <em>somewhat </em>more reasonable thanks to the psychology of <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.netsuite.com/portal/business-benchmark-brainyard/industries/articles/cfo-central/psychological-pricing.shtml&amp;xcust=2-1-1379747-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">price anchoring</a>. Nvidia and its partners are sitting on a massive stockpile of RTX 30-series GPUs after the cryptocurrency market crashed, and sunk demand for ultra-marked-up GPUs alongside it. Consider the RTX 3080: No one would expect to pay anywhere <em>near </em>its MSRP of $700 more than two years after its launch. But that&rsquo;s where we&rsquo;re at right now, and next to the $1,200 RTX 4080, it might look like a bargain, because the RTX 4080 is <em>71 percent more</em>. Nvidia is keeping both these generations alive side-by-side for a reason.</p>



<p>When Nvidia CEO Jensen Huang spoke to reporters after revealing Lovelace at GTC 2022, he said, &ldquo;The idea that a chip is going to go down in cost over time, unfortunately, is a story of the past.&rdquo; But AMD just revealed its <a href="https://www.pcworld.com/article/1370609/amd-radeon-rx-7900-xtx-5-crucial-details-pc-gamers-need-to-know.html">next-gen &ldquo;RDNA 3&rdquo; Radeon GPUs</a> and yes, they cost less than their predecessors. The Radeon RX 7900 XTX costs $999, compared to a $1,100 MSRP for the Radeon RX 6950 XT; the Radeon RX 7900 XT costs $899, compared to $999 for the Radeon RX 6900 XT (though the 7900 XT may have more in common with last-gen&rsquo;s 6800 XT).</p>



<p>We&rsquo;ll have to see where performance falls when AMD&rsquo;s cards launch on December 13, but if the Radeon RX 7900 XTX manages to hit the 1.5x to 1.7x uplift (via 6950 XT) that Radeon claims, Team Red&rsquo;s offering could very well wind up just behind the RTX 4090 and as fast or <em>faster</em> than the RTX 4080 for hundreds of dollars less.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="AMD RDNA 3 Radeon details" class="wp-image-1370468" height="670" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/radeon-rx-7900-xtx-vs-6900-4k.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /><figcaption><p>Take all vendor-supplied benchmarks with a big handful of salt, but if the Radeon RX 7900 XTX is indeed more than 1.5x faster than the 6950 XT, it could wind up faster than the 4080 in traditional games for hundreds less.</p></figcaption></figure><p class="imageCredit">AMD</p></div>



<p>Don&rsquo;t be a sucker and pay $1,200 for this card. Don&rsquo;t be a sucker and buy a 2-year-old last-gen model at full price. Either buy an RTX 4090 if you want no-compromises gaming and creation performance&mdash;or wait to see what Radeon offers this time around, or wait for Nvidia&rsquo;s prices to potentially fall after it sells through that mountain of older RTX 30-series cards. Because a 71 percent (<em>71 percent</em>) price increase for a 30 percent performance increase ain&rsquo;t it. That&rsquo;s harsh, but it&rsquo;s true.</p>



<p>Stay tuned for our full gaming review soon. In the meantime, check out Adam Taylor&rsquo;s insanely detailed evaluation of <a href="https://www.pcworld.com/article/1378383/nvidia-geforce-rtx-4080-content-creation-review.html">the GeForce RTX 4080&rsquo;s content creation chops</a>.</p>
Gaming, GPUs</div>

## Tested: Nvidiaâ€™s GeForce RTX 4080 offers dazzling creator performance, with a catch
 - [https://www.pcworld.com/article/1378383/nvidia-geforce-rtx-4080-content-creation-review.html](https://www.pcworld.com/article/1378383/nvidia-geforce-rtx-4080-content-creation-review.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 14:00:00+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>The Nvidia GeForce RTX 4080&rsquo;s slow-paced announcement and eventual launch has been&hellip; a fascinating road. From its <a href="https://www.pcworld.com/article/1354391/nvidia-cancels-the-controversial-rtx-4080-12gb.html">mis-named little brother getting &ldquo;unlaunched&rdquo;</a> to the RTX 4080 Founders Edition being the same size as <a href="https://www.pcworld.com/article/1348123/nvidia-geforce-rtx-4090-review-ada-lovelace.html">the massive RTX 4090</a> for some reason, to the <a href="https://www.pcworld.com/article/1372988/heres-what-we-know-about-the-geforce-rtx-4090-12vhpwr-cables-melting.html">melting 12VHPWR power connectors</a>&mdash;which are still on this card, yes. Nvidia has quite the PR battle with the $1,200 GeForce RTX 4080, but if you ignore all the noise, does this new card deliver the goods for content creators? </p>



<p>Thankfully, performance-wise it does mostly live up to the hype; I just find myself wishing we had higher VRAM capacities on such expensive graphics cards.</p>



<p>This review is focused on work, instead of play, similar to my <a href="https://www.pcworld.com/article/1375901/tested-nvidias-geforce-rtx-4090-is-a-content-creation-monster.html">recent RTX 4090 content creation analysis</a>. Photo editing, video editing, encoding, AI art generation, 3D and VFX workflows&mdash;stuff I use every day for my <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.youtube.com/c/EposVox&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">EposVox</a> and <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.youtube.com/channel/UCIzj4GWuxiPub8xKNBNzATA&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">analog_dreams</a> YouTube channels. Look for PCWorld&rsquo;s gaming-focused RTX 4080 review to land tomorrow, or check out our high-level holistic look at <a href="https://www.pcworld.com/article/1379747/tested-nvidia-geforce-rtx-4080-facts.html">5 must-know RTX 4080 facts</a> from our extensive gaming and creator testing right now. </p>



<p>Let&rsquo;s dig in.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure><h2>Our test setup</h2>



<p>Most of my benchmarking was performed on this test bench:</p>



<ul><li>Intel Core i9-12900k CPU</li><li>32GB Corsair Vengeance DDR5 5200MT/s RAM</li><li>ASUS ROG STRIX Z690-E Gaming Wifi Motherboard</li><li>EVGA G3 850W PSU</li><li>Source files stored on a PCIe gen 4 NVMe SSD</li></ul><p>This time, my objectives were to see how well the new RTX 4080 stood its ground against the last-generation <a href="https://www.pcworld.com/article/393472/nvidia-geforce-rtx-3080-founders-edition-review.html">GeForce RTX 3080</a> and <a href="https://www.pcworld.com/article/393531/nvidia-geforce-rtx-3090-founders-edition-review.html">RTX 3090</a>. At $1,200, the RTX 4080 needs to really out-shine the RTX 3090 (which can now be had for $1,000 to $1,200 on average)&mdash;but then if it does, the question becomes whether the RTX 4090 is warranted at all.</p>



<p>For some more hardcore testing later, as I&rsquo;ll mention, testing was done on this test bench:</p>



<ul><li>AMD Threadripper Pro 3975WX CPU</li><li>256GB Kingston ECC DDR4 RAM</li><li>ASUS WRX80 SAGE Motherboard</li><li>BeQuiet 1400W PSU</li><li>Source files stored on a PCIe gen 4 NVMe SSD</li><li>Each test featured each GPU in the same config as to not mix results.</li></ul><h2>Photo editing</h2>



<p>I&rsquo;m going to keep testing this just because there&rsquo;s constant &ldquo;little surprises&rdquo; in performance in photo apps&mdash;even if, realistically, you&rsquo;re not going to get any major gains upgrading from one high-end graphics card to another.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378409" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PSHOP-Overall.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378418" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PSHOP-General.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>The GeForce RTX 4080 tops the charts for most scores in Adobe Photoshop (other than the <a href="https://www.pcworld.com/article/543823/amd-radeon-rx-6600-review-finally-a-true-1080p-graphics-card.html">AMD Radeon RX 6600</a>&rsquo;s weirdly high performance&hellip; perhaps we need to get some more AMD cards in the house for an AMD photo editing deep dive). Photoshop was tested using the PugetBench test suite, provided by the awesome workstation builders over at <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.pugetsystems.com/&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">Puget Systems</a>.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378420" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PSHOP-GPU.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378419" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PSHOP-Filter.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>In Lightroom Classic (also using PugetBench), the RTX 4080 beats out even the 4090&rsquo;s overall score, but for most tasks, you&rsquo;ll see marginal gains over the RTX 3080 and 3090, and right up against the 4090&rsquo;s performance.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378422" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/LRC-Overall.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378427" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/LRC-PhotoMergeHDR.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378414" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/LRC-SmartPreviews.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378416" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/LRC-PhotoMergePano.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Benchmarking Affinity Photo (I know <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://affinity.serif.com/en-us/press/newsroom/affinity-v2-sets-new-standards-in-creative-software/&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">V2 just came out&hellip;</a> let&rsquo;s keep it consistent for now) the RTX 4080 slots perfectly between the RTX 3090 and 4090, as it should.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378404" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Affinity-Photo.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<h2>Video editing</h2>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378408" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PPRO-Overall.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378412" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PPRO-Export.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378421" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PPRO-LivePlayback.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Using PugetBench for Adobe Premiere Pro sees the new RTX 4080 land a fair bit higher than the RTX 3090, just under the RTX 4090. Interestingly, export and playback scores were slightly higher than the RTX 4090, meanwhile the Effects and GPU scores were lower. If you&rsquo;re doing GPU-heavy workloads at 4K and higher, you&rsquo;ll absolutely want a 4090, but most video editors will be just fine with a 4080.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378426" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PPRO-GPU.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378410" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/PPRO-Effects.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>This carries over into BlackMagic DaVinci Resolve. Resolve just got a new update to version 18.1&mdash;a <strong>massive</strong> update bringing AI audio cleanup, new fusion features and <em>finally</em> fixing fractional scaling issues on Windows, along with performance boosts.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378403" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-Overall.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Running PugetBench, we see the GeForce RTX 4080 put up pretty substantial gains&mdash;gains you would feel&mdash;over last gen&rsquo;s RTX 3080 and even the 3090. This is seriously impressive stuff. If you were considering a 3090 or RTX Titan for video editing before around the $1,000 price range, I&rsquo;d honestly consider throwing in an extra couple hundred dollars to get this boost plus AV1 encoding! If you were eying a 3080 or 3070 instead, that&rsquo;s a much tougher sell given the higher cost.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378398" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-4KMedia.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378394" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-GPUEffects.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378407" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-Fusion.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378417" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-4KExport-HEVC.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378391" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-4KExport-AV1.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Exporting a sample 4K project, the RTX 4080 and RTX 4090 run <em>laps </em>around the older cards in H265 and AV1 compared to Arc. They&rsquo;re still faster than older cards in H.264 too, but by much smaller margins.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378413" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-4KExport-H264.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Throwing the card in my Threadripper Pro build to text my intense 8K project full of 8K RAW footage, Resolve SuperScaled 4K footage, plugins and effects&hellip; unfortunately the 4080 couldn&rsquo;t export this, just like the previous 80-tier cards I tested. 16GB of VRAM still isn&rsquo;t enough, I suppose&mdash;plugins error out, color graded footage takes longer than the 4090, and so on. 8K is tough, but for $1,200 I would hope at this point to start to see higher VRAM capacities when <a href="https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-xe-hpg-review.html">Intel is handing out $350 Arc graphics cards with 16GB</a>. This is generation 3 of unchanged VRAM capacities. Strange times.</p>



<p>One of the big improvements with Resolve 18.1 is speed increases to AI-based Face Tracking and object masking. The beta build of Resolve I had access to for my RTX 4090 review was supposed to have performance boosts for this, too&mdash;but from what I can tell the Face Tracking updates weren&rsquo;t working right because it&rsquo;s significantly fasterin 18.1 compared to even that preview build.</p>



<p>The time taken to face track on 8K RAW footage in a 4K timeline dropped by 45 percent <strong>across all tested GPUs</strong>. Even the older Ampere and Turing cards moved faster for this typically <em>very slow</em> task&mdash;but Lovelace just runs away with it.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378399" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-FaceTrack.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>The RTX 4080 tracks just barely slower than the 4090, but both leave the Ampere cards in the dust. Weirdly, the RTX Titan is in a close third behind the new cards&mdash;perhaps some added AI magic in the Titan hardware, or better floating point performance advantages? I&rsquo;m not sure.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378392" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Resolve-ObjectTrack.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Object masking didn&rsquo;t improve as much with older generations, but the GeForce RTX 4080 matches the 4090 here and puts up 17 percent faster speeds than older cards. Very rad. Huge shouts out to the Resolve team for nailing such a big update, and if you do video editing every day like I do, this new generation is going to earn your time back fast. I&rsquo;m so tired of waiting on things.</p>



<h2>AI wizardry</h2>



<p>Upscaling video to 4K using Topaz Labs&rsquo; Video Enhance AI (720p upscaled to 4K using Artemis Low Quality), the GeForce RTX 4080 is a tad slower than the 4090, but still almost a minute faster than the RTX 3090. It&rsquo;s still a little too slow for me to be comfortable using it on the regular for quick-turnaround projects, but we&rsquo;re getting there!</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378405" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Topaz-VEAI.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Similar results are found in Gigapixel. Upscaling 8 images shows the RTX 4080 running 10 seconds slower than the 4090, but still a fair bit faster than anything else.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378406" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Topaz-Gigapixel.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>PhotoAI needed to be completely re-tested. Remember how terrible Nvidia cards performed in <a href="https://www.pcworld.com/article/1375901/tested-nvidias-geforce-rtx-4090-is-a-content-creation-monster.html">my RTX 4090 review</a>? Apparently there was a big bug causing that, and Topaz has now fixed it. The RTX 4080 performs the same as the 4090 here, but both cards take less than a <em>third</em> of the time the older cards take. This is the kind of AI performance improvements I was expecting to see with Lovelace!</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378401" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Topaz-PhotoAI.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Generating AI art with Stable Diffusion shows the RTX 4080 being a little slower than the RTX 3090 in general, but still beating out the RTX Titan and 3080. I&rsquo;m told by people in these communities that there&rsquo;s probably some optimization that can be done in the apps for Stable to optimize specifically for Lovelace, so hopefully we see that soon. <em>(Who do I send a card to?)</em></p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378396" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/StableDiffusion-HalfPrec.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation" class="wp-image-1378431" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/StableDiffusion-FullPrec.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Using FlowFrames to AI interpolate a 60FPS video into 120FPS for slowmo, the RTX 4080 is just a little faster than the RTX 3090. Nice to see, <em>but</em> Lovelace has hardware built-in to do frame interpolation in real-time with DLSS3 Frame Generation&mdash;there simply isn&rsquo;t any software support for this outside of games. (For video, this is a feature called <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://developer.nvidia.com/blog/av1-encoding-and-fruc-video-performance-boosts-and-higher-fidelity-on-the-nvidia-ada-architecture/&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">FRUC</a>.) I would <strong>love</strong> to see integrations with DaVinci Resolve for applying this to footage on the timeline, as well as in OBS Studio for smoothing out stuttery captures. A guy can dream, I guess<em>.</em></p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation" class="wp-image-1378432" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/FlowFrames-1.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<h2>3D &amp; VFX</h2>



<p>If you&rsquo;re a 3D artist or VFX artist using 3D rendering, Nvidia&rsquo;s GeForce RTX 4080 is no slouch, either. Now, the bigger, badder RTX 4090 put up insane numbers, doubling the 3090&rsquo;s performance in ways that you can&rsquo;t fully expect with this step-down GPU&mdash;but it&rsquo;s close!</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378402" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Blender-Monster.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>In Blender, benchmarking the Monster scene with the RTX 4080 nets about 60 percent faster speed over the RTX 3090 and 85 percent faster than the 3080. The Junkshop scene benched about 29 percent faster than the 3090 and 50 percent faster than the 3080, while the classic Classroom scene shows 55 percent faster performance than the 3090 and 82 percent faster than the 3080. So while the GeForce RTX 4080 was never going to be as absurdly fast as the 4090, these are some very good gains.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378411" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Blender-Junkshop.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378395" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Blender-Classroom.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>The trend continues into Octane, where the RTX 4080 performs 42 percent faster than the 4090 and 67 percent faster than the 3080. And just for kicks, if you&rsquo;re still on the RTX Titan&hellip; nearly 3X the performance. I&rsquo;m so sorry that card once cost $2500.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378397" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/Octane.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<p>Testing the V-Ray engine with the VRAY 5 benchmark is an oddity. The VRAY RTX test shows the 4080 still trailing behind the RTX 4090 a fair bit, but leaping substantially over the older generation GPUs, but in CUDA? For some reason I&rsquo;m consistently scoring <em>below</em> those older cards in performance. I can&rsquo;t explain this, a bug, given all the other performance results we&rsquo;re seeing here.</p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image size-large"><img alt="GeForce RTX 4080 content creation performance" class="wp-image-1378390" height="675" src="https://b2c-contenthub.com/wp-content/uploads/2022/11/VRAY.jpg?quality=50&amp;strip=all&amp;w=1200" width="1200" /></figure><p class="imageCredit">Adam Taylor/IDG</p></div>



<h2><a></a>Streaming</h2>



<p>All GeForce RTX 40-series cards have dual video encoder chips to allow for faster and higher quality 4K and 8K recording, as well as 8K 60FPS capture (Ampere caps out at 8K 30FPS). You can read more about this in the <a href="https://www.pcworld.com/article/1375901/tested-nvidias-geforce-rtx-4090-is-a-content-creation-monster.html#:~:text=encoding%20speeds%20yet.-,Video%20encoding,-For%20simply%20transcoding">streaming section of my RTX 4090 review</a>, but this has been fantastic for my workflow and has me putting 40-series cards in all of my capture and streaming rigs.</p>



<p>I&rsquo;m recording most things to AV1 now to save on space. Resolve supports editing it without issue now, and the <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://www.capframex.com/tests/AV1%20Video%20Decoding%20on%20Intel%20Arc%20A770&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">latest Nvidia driver update just improved AV1 decode performance</a>, too! I&rsquo;m utilizing the extra overall headroom to record my screen captures in H.265 in 4:4:4 chroma subsampling for lossless punch-ins for my tutorials and for beautiful 8K upscales on export. You can <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://youtu.be/8VtO2hRT0no&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">view a demo of this here</a>&mdash;screen capture never looked so good, and I could not do this without risking frame drops and encoder lag even on my RTX 3090.</p>



<p>As with the RTX 4090, recording AV1 in OBS Studio 28.1 (which <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://youtu.be/8SPT5Ppenfs&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">now supports NVENC AV1</a> in the public build) has very minimal performance impact on your gameplay, a mere fraction of the hit that H.264 causes. I&rsquo;ll have more deeper testing on this coming later./y</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure><p>Discord is also getting AV1 streaming support for Go Live later <a href="https://go.redirectingat.com/?id=111346X1569483&amp;url=https://github.com/Discord-Datamining/Discord-Datamining/commit/683e32f7894389e7b9673b336efcd94956cfa90b#commitcomment-89434143&amp;xcust=2-1-1378383-1-0-0&amp;sref=https://www.pcworld.com/feed" rel="nofollow">this month</a>. I tested this and it looks great, but there&rsquo;s some quirks to how Discord calls work. Since it&rsquo;s a &ldquo;video call,&rdquo; video is sent directly from host to viewers, with no transcoding server or much of a middle-man as you would have with services such as Twitch and YouTube. This means that both parties have to have the same support.</p>



<p>So if you&rsquo;re streaming AV1 to Discord and a viewer joins who doesn&rsquo;t have the right driver or hardware (at this time I&rsquo;m unclear as to whether it will require Nvidia cards for this update, or just GPU AV1 decode support in general, which would include AMD and Intel), the stream will be forced to revert to H.264. This could be confusing and messy for group calls <em>but</em> is still a big win to have it integrated. I just hope we see Arc and <a href="https://www.pcworld.com/article/1370428/amd-rdna-3-radeon-rx-7900-xtx-reveal.html">Radeon RDNA3 AV1 support</a> added sooner rather than later&mdash;and ideally SVT-AV1 CPU encoding support, as it&rsquo;s gotten pretty fast.</p>



<h2>Bottom Line: Nvidia&rsquo;s product stack is confusing</h2>



<p>While it felt like a win for Nvidia to &ldquo;unlaunch&rdquo; the 12GB 4080&mdash;since it never should have been called a 4080 in the first place&mdash;doing so makes it so much harder to understand Nvidia&rsquo;s upcoming product stack in context. The GeForce RTX 4080 out-performs the last-gen RTX 3090 in many cases, which seems like a win for $1,200, but you&rsquo;re getting shorted on VRAM in ways that do start to matter for high-end work.</p>



<p>Just looking up to the $1,600 GeForce RTX 4090, the 4080&rsquo;s performance checks out. But the cheaper 12GB&hellip; 4070? 4070 Ti? 4080 mini?&hellip; that will eventually hit the streets might be mostly just as good for this kind of work for much cheaper, given that the RTX 4080&rsquo;s 16GB of VRAM still limits super-high-end work. But we won&rsquo;t know until Nvidia&rsquo;s next products break cover.</p>



<p>It&rsquo;s hard to suggest buying anything right now until this painfully slow season is over&mdash;but waiting puts you in conflict with scalpers and shortages and FOMO. It&rsquo;s tough. I was honestly hoping the &ldquo;unlaunch&rdquo; scenario would drive down the price of both RTX 4080 cards a bit, but I guess that&rsquo;s not happening. Either way, if your particular content creation workflows don&rsquo;t require more than 16GB of onboard VRAM, Nvidia&rsquo;s GeForce RTX 4080 delivers some stunning performance improvements over its predecessors.</p>
GPUs</div>

## These JBL Bluetooth earbuds are just $50 with this early Black Friday discount
 - [https://www.pcworld.com/article/1378698/these-jbl-bluetooth-earbuds-are-just-50-with-this-early-black-friday-discount.html](https://www.pcworld.com/article/1378698/these-jbl-bluetooth-earbuds-are-just-50-with-this-early-black-friday-discount.html)
 - RSS feed: https://www.pcworld.com/index.rss
 - date published: 2022-11-15 08:00:00+00:00
 - Starred: False

<div id="link_wrapped_content">
<section class="wp-block-bigbite-multi-title"><div class="container"></div></section><p>Noise-canceling headphones are a necessity for anyone that works in a busy office. The only problem? They&rsquo;re often pretty expensive. This is why, if you want&nbsp;<a href="https://shop.pcworld.com/sales/jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones?utm_source=pcworld.com&amp;utm_medium=referral&amp;utm_campaign=jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones&amp;utm_term=scsf-557280&amp;utm_content=a0x1P000004yYqiQAE&amp;scsonar=1" rel="noreferrer noopener" target="_blank">budget-friendly Bluetooth earbuds</a>, consider&nbsp;the JBL Live Free NC+, which is on sale for just $49.99 during our Every Friday is Black Friday Sale while supplies last.</p>



<p>JBL Live Free NC+ earbuds provide an excellent,&nbsp;<a href="https://shop.pcworld.com/sales/jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones?utm_source=pcworld.com&amp;utm_medium=referral&amp;utm_campaign=jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones&amp;utm_term=scsf-557280&amp;utm_content=a0x1P000004yYqiQAE&amp;scsonar=1" rel="noreferrer noopener" target="_blank">distraction-free listening experience</a>. They feature active noise canceling technology that filters out external sounds so they won&rsquo;t detract from your music. But, since you probably don&rsquo;t want to cut yourself off from the world entirely, they also allow certain ambient sounds &mdash; like conversations &mdash; to pass through. Verified customer John T. liked how he could, &ldquo;hear what is going on around me but the volume of the fans, radios and conversations I&rsquo;m not listening to are muted.&rdquo;</p>



<p>That means you&rsquo;ll be aware when your boss wants your attention, which is probably good if you value your job.&nbsp;Beyond that, they offer a long battery life, they&rsquo;re Qi-compatible, and they&rsquo;re waterproof, too, so you can wear them out in the elements.</p>



<p>So while they didn&rsquo;t make&nbsp;PCWorld&rsquo;s list&nbsp;of the best hardware for this year, we think they should get an honorable mention. This week, you can get the&nbsp;<a href="https://shop.pcworld.com/sales/jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones?utm_source=pcworld.com&amp;utm_medium=referral&amp;utm_campaign=jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones&amp;utm_term=scsf-557280&amp;utm_content=a0x1P000004yYqiQAE&amp;scsonar=1" rel="noreferrer noopener" target="_blank">JBL Live Free NC+</a>&nbsp;for $49.99, the lowest price on the web. No coupons are necessary, but stock is limited, so once we sell out, they&rsquo;re gone for good!</p>



<p><a href="https://shop.pcworld.com/sales/jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones?utm_source=pcworld.com&amp;utm_medium=referral-cta&amp;utm_campaign=jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones&amp;utm_term=scsf-557280&amp;utm_content=a0x1P000004yYqiQAE&amp;scsonar=1" rel="noreferrer noopener" target="_blank">&nbsp;</a></p>


<div class="extendedBlock-wrapper block-coreImage undefined"><figure class="wp-block-image"><img alt="" src="https://cdnp3.stackassets.com/653e06e5d5b08a6a90f1bf8badc6996c706ef03e/store/0d854f1c74a5aa2182f7f79ec75880b7944d0965c0cfba82cfb3f09b2c9b/sale_313088_primary_image.jpg" /></figure></div>



<p><strong>JBL Live Free NC+ True Wireless in-Ear Noise Cancelling Bluetooth Earbuds &ndash; $49.99</strong></p>



<p><a href="https://shop.pcworld.com/sales/jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones?utm_source=pcworld.com&amp;utm_medium=referral-cta&amp;utm_campaign=jbl-live-free-nc-true-wireless-in-ear-noise-cancelling-bluetooth-headphones&amp;utm_term=scsf-557280&amp;utm_content=a0x1P000004yYqiQAE&amp;scsonar=1" rel="noreferrer noopener" target="_blank">Block Out the Noise</a></p>



<p><em>Prices subject to change.</em></p>
Headphones, JBL</div>
